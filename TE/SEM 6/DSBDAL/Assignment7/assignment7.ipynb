{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1:  Hey, did you know that the summer break is coming? Amazing right!! It’s only 5 more days!!\n",
      "Text2:  They wandered into a strange Tiki bar on the edge of the small beach town. Every manager should be able to recite at least ten nursery rhymes backward. Find bar near beach\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import nltk\n",
    "\n",
    "text1 = \"Hey, did you know that the summer break is coming? Amazing right!! It’s only 5 more days!!\"\n",
    "text2 = \"They wandered into a strange Tiki bar on the edge of the small beach town. Every manager should be able to recite at least ten nursery rhymes backward. Find bar near beach\"\n",
    "print(\"Text1: \", text1)\n",
    "print(\"Text2: \", text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/TE/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization of Text1:  ['Hey', ',', 'did', 'you', 'know', 'that', 'the', 'summer', 'break', 'is', 'coming', '?', 'Amazing', 'right', '!', '!', 'It', '’', 's', 'only', '5', 'more', 'days', '!', '!']\n",
      "Word Tokenization of Text2:  ['They', 'wandered', 'into', 'a', 'strange', 'Tiki', 'bar', 'on', 'the', 'edge', 'of', 'the', 'small', 'beach', 'town', '.', 'Every', 'manager', 'should', 'be', 'able', 'to', 'recite', 'at', 'least', 'ten', 'nursery', 'rhymes', 'backward', '.', 'Find', 'bar', 'near', 'beach']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "# 1. Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(\"Text1: \", text1)\n",
    "print(\"Word Tokenization of Text1: \", word_tokenize(text1))\n",
    "print(\"Text2: \", text2)\n",
    "print(\"Word Tokenization of Text2: \", word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization of Text1:  ['Hey, did you know that the summer break is coming?', 'Amazing right!!', 'It’s only 5 more days!', '!']\n",
      "Word Tokenization of Text2:  ['They wandered into a strange Tiki bar on the edge of the small beach town.', 'Every manager should be able to recite at least ten nursery rhymes backward.', 'Find bar near beach']\n"
     ]
    }
   ],
   "source": [
    "# 2. Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print(\"Text1: \", text1)\n",
    "print(\"Word Tokenization of Text1: \", sent_tokenize(text1))\n",
    "print(\"Text2: \", text2)\n",
    "print(\"Word Tokenization of Text2: \", sent_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/TE/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/TE/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging of Text1:  [('Hey', 'NNP'), (',', ','), ('know', 'VBP'), ('summer', 'NN'), ('break', 'NN'), ('coming', 'VBG'), ('?', '.')]\n",
      "POS Tagging of Text2:  [('They', 'PRP'), ('wandered', 'VBD'), ('strange', 'JJ'), ('Tiki', 'NNP'), ('bar', 'NN'), ('edge', 'NN'), ('small', 'JJ'), ('beach', 'NN'), ('town', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def pos_tagging(text):\n",
    "    tokenized = sent_tokenize(text)\n",
    "    for i in tokenized:\n",
    "        # Word tokenizers is used to find the words\n",
    "        # and punctuation in a string\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "    \n",
    "        # removing stop words from wordList\n",
    "        wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    \n",
    "        #  Using a Tagger. Which is part-of-speech\n",
    "        # tagger or POS-tagger.\n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "    \n",
    "        return tagged\n",
    "    \n",
    "print(\"Text1: \", text1)\n",
    "print(\"POS Tagging of Text1: \", pos_tagging(text1))\n",
    "print(\"Text2: \", text2)\n",
    "print(\"POS Tagging of Text2: \", pos_tagging(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1:  Hey, did you know that the summer break is coming? Amazing right!! It’s only 5 more days!!\n",
      "['Hey', ',', 'did', 'you', 'know', 'that', 'the', 'summer', 'break', 'is', 'coming', '?', 'Amazing', 'right', '!', '!', 'It', '’', 's', 'only', '5', 'more', 'days', '!', '!']\n",
      "['Hey', ',', 'know', 'summer', 'break', 'coming', '?', 'Amazing', 'right', '!', '!', '’', '5', 'days', '!', '!']\n",
      "Text1:  Hey, did you know that the summer break is coming? Amazing right!! It’s only 5 more days!!\n",
      "['Hey', ',', 'did', 'you', 'know', 'that', 'the', 'summer', 'break', 'is', 'coming', '?', 'Amazing', 'right', '!', '!', 'It', '’', 's', 'only', '5', 'more', 'days', '!', '!']\n",
      "['Hey', ',', 'know', 'summer', 'break', 'coming', '?', 'Amazing', 'right', '!', '!', 'It', '’', '5', 'days', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "  \n",
    "word_tokens = word_tokenize(text1)\n",
    "\n",
    "# With LowerCase Conversion\n",
    "# converts the words in word_tokens to lower case and then checks whether they are present in stop_words or not\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "print(\"Text1: \", text1)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "  \n",
    "word_tokens = word_tokenize(text1)\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)\n",
    "  \n",
    "# With no lower case conversion\n",
    "  \n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(\"Text1: \", text1)\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/TE/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey  :  hey\n",
      ",  :  ,\n",
      "did  :  did\n",
      "you  :  you\n",
      "know  :  know\n",
      "that  :  that\n",
      "the  :  the\n",
      "summer  :  summer\n",
      "break  :  break\n",
      "is  :  is\n",
      "coming  :  come\n",
      "?  :  ?\n",
      "Amazing  :  amaz\n",
      "right  :  right\n",
      "!  :  !\n",
      "!  :  !\n",
      "It  :  it\n",
      "’  :  ’\n",
      "s  :  s\n",
      "only  :  onli\n",
      "5  :  5\n",
      "more  :  more\n",
      "days  :  day\n",
      "!  :  !\n",
      "!  :  !\n",
      "Stemming of Text1:  None\n",
      "They  :  they\n",
      "wandered  :  wander\n",
      "into  :  into\n",
      "a  :  a\n",
      "strange  :  strang\n",
      "Tiki  :  tiki\n",
      "bar  :  bar\n",
      "on  :  on\n",
      "the  :  the\n",
      "edge  :  edg\n",
      "of  :  of\n",
      "the  :  the\n",
      "small  :  small\n",
      "beach  :  beach\n",
      "town  :  town\n",
      ".  :  .\n",
      "Every  :  everi\n",
      "manager  :  manag\n",
      "should  :  should\n",
      "be  :  be\n",
      "able  :  abl\n",
      "to  :  to\n",
      "recite  :  recit\n",
      "at  :  at\n",
      "least  :  least\n",
      "ten  :  ten\n",
      "nursery  :  nurseri\n",
      "rhymes  :  rhyme\n",
      "backward  :  backward\n",
      ".  :  .\n",
      "Find  :  find\n",
      "bar  :  bar\n",
      "near  :  near\n",
      "beach  :  beach\n",
      "Stemming of Text2:  None\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    for w in words:\n",
    "        print(w, \" : \", ps.stem(w))\n",
    "\n",
    "print(\"Text1: \", text1)\n",
    "print(\"Stemming of Text1: \", stemming(text1))\n",
    "print(\"Text2: \", text2)\n",
    "print(\"Stemming of Text2: \", stemming(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1:  Hey, did you know that the summer break is coming? Amazing right!! It’s only 5 more days!!\n",
      "Lemmatization of Text1:  ['Hey', ',', 'do', 'you', 'know', 'that', 'the', 'summer', 'break', 'be', 'come', '?', 'Amazing', 'right', '!', '!', 'It', '’', 's', 'only', '5', 'more', 'days', '!', '!']\n",
      "Text2:  They wandered into a strange Tiki bar on the edge of the small beach town. Every manager should be able to recite at least ten nursery rhymes backward. Find bar near beach\n",
      "Lemmatization of Text2:  ['They', 'wander', 'into', 'a', 'strange', 'Tiki', 'bar', 'on', 'the', 'edge', 'of', 'the', 'small', 'beach', 'town', '.', 'Every', 'manager', 'should', 'be', 'able', 'to', 'recite', 'at', 'least', 'ten', 'nursery', 'rhyme', 'backward', '.', 'Find', 'bar', 'near', 'beach']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# lemmatize string\n",
    "def lemmatization(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # provide context i.e. part-of-speech\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "print(\"Text1: \", text1)\n",
    "print(\"Lemmatization of Text1: \", lemmatization(text1))\n",
    "print(\"Text2: \", text2)\n",
    "print(\"Lemmatization of Text2: \", lemmatization(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1:  Hey, did you know that the summer break is coming? Amazing right!! It’s only 5 more days!!\n",
      "Term Frequency for text 1\n",
      "{'Hey': 0.058823529411764705, ',': 0.058823529411764705, 'know': 0.058823529411764705, 'summer': 0.058823529411764705, 'break': 0.058823529411764705, 'coming': 0.058823529411764705, '?': 0.058823529411764705, 'Amazing': 0.058823529411764705, 'right': 0.058823529411764705, '!': 0.23529411764705882, 'It': 0.058823529411764705, '’': 0.058823529411764705, '5': 0.058823529411764705, 'days': 0.058823529411764705}\n",
      "\n",
      "Text2:  They wandered into a strange Tiki bar on the edge of the small beach town. Every manager should be able to recite at least ten nursery rhymes backward. Find bar near beach\n",
      "Term Frequency for text 2\n",
      "{'They': 0.041666666666666664, 'wandered': 0.041666666666666664, 'strange': 0.041666666666666664, 'Tiki': 0.041666666666666664, 'bar': 0.08333333333333333, 'edge': 0.041666666666666664, 'small': 0.041666666666666664, 'beach': 0.08333333333333333, 'town': 0.041666666666666664, '.': 0.08333333333333333, 'Every': 0.041666666666666664, 'manager': 0.041666666666666664, 'able': 0.041666666666666664, 'recite': 0.041666666666666664, 'least': 0.041666666666666664, 'ten': 0.041666666666666664, 'nursery': 0.041666666666666664, 'rhymes': 0.041666666666666664, 'backward': 0.041666666666666664, 'Find': 0.041666666666666664, 'near': 0.041666666666666664}\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens1 = word_tokenize(text1)\n",
    "filtered1 = []\n",
    "for w in word_tokens1:\n",
    "    if w not in stop_words:\n",
    "        filtered1.append(w)\n",
    "word_tokens2 = word_tokenize(text2)\n",
    "filtered2 = []\n",
    "for w in word_tokens2:\n",
    "    if w not in stop_words:\n",
    "        filtered2.append(w)\n",
    "term_frequency1 = {}\n",
    "term_frequency2 = {}\n",
    "for i in filtered1:\n",
    "    term_frequency1.update({i:(filtered1.count(i))/len(filtered1)})\n",
    "for i in filtered2:\n",
    "    term_frequency2.update({i:(filtered2.count(i))/len(filtered2)})\n",
    "\n",
    "print(\"Text1: \", text1)\n",
    "print(\"Term Frequency for text 1\")\n",
    "print(term_frequency1)\n",
    "print(\"\\nText2: \", text2)\n",
    "print(\"Term Frequency for text 2\")\n",
    "print(term_frequency2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1:  Hey, did you know that the summer break is coming? Amazing right!! It’s only 5 more days!!\n",
      "Inverse Document frequency of Document1\n",
      "{'Hey': 0.6931471805599453, ',': 0.6931471805599453, 'know': 0.6931471805599453, 'summer': 0.6931471805599453, 'break': 0.6931471805599453, 'coming': 0.6931471805599453, '?': 0.6931471805599453, 'Amazing': 0.6931471805599453, 'right': 0.6931471805599453, '!': 0.6931471805599453, 'It': 0.6931471805599453, '’': 0.6931471805599453, '5': 0.6931471805599453, 'days': 0.6931471805599453}\n",
      "\n",
      "Text2:  They wandered into a strange Tiki bar on the edge of the small beach town. Every manager should be able to recite at least ten nursery rhymes backward. Find bar near beach\n",
      "\n",
      "Inverse Document frequency of Document2\n",
      "{'They': 0.6931471805599453, 'wandered': 0.6931471805599453, 'strange': 0.6931471805599453, 'Tiki': 0.6931471805599453, 'bar': 0.6931471805599453, 'edge': 0.6931471805599453, 'small': 0.6931471805599453, 'beach': 0.6931471805599453, 'town': 0.6931471805599453, '.': 0.6931471805599453, 'Every': 0.6931471805599453, 'manager': 0.6931471805599453, 'able': 0.6931471805599453, 'recite': 0.6931471805599453, 'least': 0.6931471805599453, 'ten': 0.6931471805599453, 'nursery': 0.6931471805599453, 'rhymes': 0.6931471805599453, 'backward': 0.6931471805599453, 'Find': 0.6931471805599453, 'near': 0.6931471805599453}\n"
     ]
    }
   ],
   "source": [
    "# Inverse Document Frequency\n",
    "import math\n",
    "ifd1 = {}\n",
    "ifd2 = {}\n",
    "for i in filtered1:\n",
    "    cnt = 0\n",
    "    if filtered1.count(i) > 0:\n",
    "        cnt += 1\n",
    "    if filtered2.count(i) > 0:\n",
    "        cnt += 1\n",
    "    f = math.log(2/cnt)\n",
    "    ifd1.update({i:f})\n",
    "    \n",
    "for i in filtered2:\n",
    "    cnt = 0\n",
    "    if filtered1.count(i) > 0:\n",
    "        cnt += 1\n",
    "    if filtered2.count(i) > 0:\n",
    "        cnt += 1\n",
    "    f = math.log(2/cnt)\n",
    "    ifd2.update({i:f})\n",
    "\n",
    "print(\"Text1: \", text1)\n",
    "print(\"Inverse Document frequency of Text1\")\n",
    "print(ifd1)\n",
    "print(\"\\nText2: \", text2)\n",
    "print(\"\\nInverse Document frequency of Text2\")\n",
    "print(ifd2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
